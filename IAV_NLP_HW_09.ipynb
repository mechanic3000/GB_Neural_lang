{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mechanic3000/GB_Neural_lang/blob/Lesson_09/IAV_NLP_HW_09.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yG_n40gFzf9s"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-an5tHuaRmqD"
      },
      "source": [
        "path_to_file = '/content/Понедельник начинается в субботу.TXT'"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aavnuByVymwK",
        "outputId": "291ec6fb-9c35-466d-a0cc-8bcbb9821fb7"
      },
      "source": [
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "# length of text is the number of characters in it\n",
        "print('Length of text: {} characters'.format(len(text)))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 439505 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Duhg9NrUymwO",
        "outputId": "8deaa4cb-7ffe-410e-d0ed-254c5325c52d"
      },
      "source": [
        "print(text[2000:2500])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "днем сиденье.\r\n",
            "     -- Благодетель!  --  обрадованно произнес горбоносый,  снял с плеча\r\n",
            "ружье и сел рядом со мной.\r\n",
            "     Бородатый, нерешительно  заглядывая в заднюю дверцу,  сказал:\r\n",
            "     -- А можно, я здесь немножко того?..\r\n",
            "     Я перегнулся  через  спинку  и помог ему расчистить место,  занятое\r\n",
            "спальным мешком и свернутой  палаткой.  Он  деликатно  уселся,  поставив\r\n",
            "ружье между коленей.\r\n",
            "     -- Дверцу прикройте получше, -- сказал я.\r\n",
            "     Все шло как обычно. Машина тронулась. Горбоносый \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jtNGSzKn1o-6"
      },
      "source": [
        "text = text + text"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IlCgQBRVymwR",
        "outputId": "600c7e9b-6f4b-49cf-bd58-e82fb64740f3"
      },
      "source": [
        "# The unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "print('{} unique characters'.format(len(vocab)))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "113 unique characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IalZLbvOzf-F"
      },
      "source": [
        "# Creating a mapping from unique characters to indices\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "text_as_int = np.array([char2idx[c] for c in text])"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p-DhY8bbTY3g",
        "outputId": "6cbc0454-9823-4af3-fe56-888f5dd51852"
      },
      "source": [
        "text_as_int, text[:500], len(text_as_int), len(text)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([50, 97, 91, ...,  0,  1,  0]),\n",
              " 'Аркадий и Борис Стругацкие. \\r\\nПонедельник начинается в субботу\\r\\n\\r\\n\\r\\n     Повесть-сказка для научных работников младшего возраста\\r\\n\\r\\n                             Но что странное, что непонятнее всего,\\r\\n                             это то, как авторы могут брать подобные\\r\\n                             сюжеты, признаюсь, это уж совсем\\r\\n                             непостижимо, это точно... нет, нет,\\r\\n                             совсем не понимаю.\\r\\n                                                  Н',\n",
              " 879010,\n",
              " 879010)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgsVvVxnymwf"
      },
      "source": [
        "### train and target"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0UHJDA39zf-O",
        "outputId": "c1b9d04f-69f0-40ec-af0f-c2da5ed21b58"
      },
      "source": [
        "# The maximum length sentence you want for a single input in characters\n",
        "seq_length = 100\n",
        "examples_per_epoch = len(text)//(seq_length+1)\n",
        "\n",
        "# Create training examples / targets\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "\n",
        "for i in char_dataset.take(5):\n",
        "    print(idx2char[i.numpy()])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "А\n",
            "р\n",
            "к\n",
            "а\n",
            "д\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l4hkDU3i7ozi",
        "outputId": "d325f984-7359-454e-a6ad-eea20b594ba0"
      },
      "source": [
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for item in sequences.take(5):\n",
        "    print(repr(''.join(idx2char[item.numpy()])))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'Аркадий и Борис Стругацкие. \\r\\nПонедельник начинается в субботу\\r\\n\\r\\n\\r\\n     Повесть-сказка для научных р'\n",
            "'аботников младшего возраста\\r\\n\\r\\n                             Но что странное, что непонятнее всего,\\r\\n '\n",
            "'                            это то, как авторы могут брать подобные\\r\\n                             сюж'\n",
            "'еты, признаюсь, это уж совсем\\r\\n                             непостижимо, это точно... нет, нет,\\r\\n    '\n",
            "'                         совсем не понимаю.\\r\\n                                                  Н.В.Го'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9NGu-FkO_kYU"
      },
      "source": [
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiCopyGZymwi"
      },
      "source": [
        "Print the first example input and target values:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GNbw-iR0ymwj",
        "outputId": "b0af9042-1e46-4dd9-8667-b41619cf5b79"
      },
      "source": [
        "for input_example, target_example in  dataset.take(2):\n",
        "    print('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
        "    print('Target data:', repr(''.join(idx2char[target_example.numpy()])))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input data:  'Аркадий и Борис Стругацкие. \\r\\nПонедельник начинается в субботу\\r\\n\\r\\n\\r\\n     Повесть-сказка для научных '\n",
            "Target data: 'ркадий и Борис Стругацкие. \\r\\nПонедельник начинается в субботу\\r\\n\\r\\n\\r\\n     Повесть-сказка для научных р'\n",
            "Input data:  'аботников младшего возраста\\r\\n\\r\\n                             Но что странное, что непонятнее всего,\\r\\n'\n",
            "Target data: 'ботников младшего возраста\\r\\n\\r\\n                             Но что странное, что непонятнее всего,\\r\\n '\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p2pGotuNzf-S",
        "outputId": "e5d4d076-1c10-4ea4-a295-82543f9c61b7"
      },
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "dataset"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_BatchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHT8cLh7EAsg"
      },
      "source": [
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZUzZLkyC1UpP"
      },
      "source": [
        "# model = tf.keras.Sequential(\n",
        "#     [\n",
        "#         tf.keras.layers.Embedding(vocab_size, embedding_dim),\n",
        "#         tf.keras.layers.LSTM(rnn_units, return_sequences=True),\n",
        "#         tf.keras.layers.Dense(vocab_size)\n",
        "#     ]\n",
        "# )"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tm1u0iNSaLOi"
      },
      "source": [
        "# class RNNgenerator(tf.keras.Model):\n",
        "#     def __init__(self, vocab_size, embedding_dim, batch_size):\n",
        "#         super(RNNgenerator, self).__init__()\n",
        "        \n",
        "#         self.emb = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "                                 \n",
        "#         self.gru1 = tf.keras.layers.GRU(rnn_units,\n",
        "#                             return_sequences=True,\n",
        "#                             recurrent_initializer='glorot_uniform')\n",
        "#         self.gru2 = tf.keras.layers.GRU(rnn_units,\n",
        "#                             return_sequences=True,\n",
        "#                             recurrent_initializer='glorot_uniform')\n",
        "                           \n",
        "#         self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "#     def call(self, x):\n",
        "#         emb_x = self.emb(x)\n",
        "#         x1 = self.gru1(emb_x)\n",
        "#         x = x1\n",
        "#         for _ in range(3):\n",
        "#             x = self.gru2(x)\n",
        "#         #x = self.gru1(x)\n",
        "#         x = (x + x1)/2\n",
        "#         return self.fc(x)\n",
        "\n",
        "# model = RNNgenerator(vocab_size, embedding_dim, BATCH_SIZE)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MtCrdfzEI2N0"
      },
      "source": [
        "# def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "#     model = tf.keras.Sequential([\n",
        "#         tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "#                                   batch_input_shape=[batch_size, None]),\n",
        "                                 \n",
        "#         tf.keras.layers.LSTM(rnn_units,\n",
        "#                             return_sequences=True,\n",
        "#                             stateful=True,\n",
        "#                             recurrent_initializer='glorot_uniform'),\n",
        "\n",
        "#         tf.keras.layers.LSTM(rnn_units,\n",
        "#                             return_sequences=True,\n",
        "#                             stateful=True,\n",
        "#                             recurrent_initializer='glorot_uniform'),\n",
        "\n",
        "#          tf.keras.layers.LSTM(rnn_units,\n",
        "#                             return_sequences=True,\n",
        "#                             stateful=True,\n",
        "#                             recurrent_initializer='glorot_uniform'),\n",
        "        \n",
        "#         tf.keras.layers.LSTM(rnn_units,\n",
        "#                             return_sequences=True,\n",
        "#                             stateful=True,\n",
        "#                             recurrent_initializer='glorot_uniform'),\n",
        "                                   \n",
        "#         tf.keras.layers.Dense(vocab_size)\n",
        "#     ])\n",
        "#     return model"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FNfDlxWZ8NXK"
      },
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Embedding(vocab_size, embedding_dim),\n",
        "                                 \n",
        "        tf.keras.layers.GRU(rnn_units,\n",
        "                            return_sequences=True,\n",
        "                            stateful=False,\n",
        "                            recurrent_initializer='glorot_uniform'),\n",
        "\n",
        "        tf.keras.layers.GRU(rnn_units,\n",
        "                            return_sequences=True,\n",
        "                            stateful=False,\n",
        "                            recurrent_initializer='glorot_uniform'),\n",
        "\n",
        "         tf.keras.layers.GRU(rnn_units,\n",
        "                            return_sequences=True,\n",
        "                            stateful=False,\n",
        "                            recurrent_initializer='glorot_uniform'),\n",
        "                                   \n",
        "        tf.keras.layers.Dense(vocab_size)\n",
        "    ])\n",
        "    return model"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwsrpOik5zhv"
      },
      "source": [
        "model = build_model(\n",
        "    vocab_size=len(vocab),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units,\n",
        "    batch_size=BATCH_SIZE)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-_70kKAPrPU",
        "outputId": "5571971f-8eb3-45f6-f748-0933f6ff0776"
      },
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 113) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vPGmAAXmVLGC",
        "outputId": "57587aea-f817-443f-88a1-a8de96b3cb8f"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, None, 256)         28928     \n",
            "                                                                 \n",
            " gru (GRU)                   (None, None, 1024)        3938304   \n",
            "                                                                 \n",
            " gru_1 (GRU)                 (None, None, 1024)        6297600   \n",
            "                                                                 \n",
            " gru_2 (GRU)                 (None, None, 1024)        6297600   \n",
            "                                                                 \n",
            " dense (Dense)               (None, None, 113)         115825    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 16,678,257\n",
            "Trainable params: 16,678,257\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJL0Q0YPY6Ee"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4HrXTACTdzY-",
        "outputId": "b2712001-6cc1-4f1d-f4cc-58d9e90e1413"
      },
      "source": [
        "def loss(labels, logits):\n",
        "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "example_batch_loss = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape:  (64, 100, 113)  # (batch_size, sequence_length, vocab_size)\n",
            "scalar_loss:       4.7275906\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDl1_Een6rL0"
      },
      "source": [
        "model.compile(optimizer='adam', loss=loss, run_eagerly=True)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ieSJdchZggUj"
      },
      "source": [
        "### Configure checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tINIEZEzLH1C"
      },
      "source": [
        "!rm -rf ./training_checkpoints"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q71A8AWiOMa7",
        "outputId": "e3b0c31a-6fc4-4490-ec37-828f87572382"
      },
      "source": [
        "!ls ./training_checkpoints"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ls: cannot access './training_checkpoints': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6fWTriUZP-n"
      },
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_freq=88*3,\n",
        "    save_weights_only=True)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Ky3F_BhgkTW"
      },
      "source": [
        "### Execute the training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yGBE2zxMMHs"
      },
      "source": [
        "EPOCHS = 20"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UK-hmKjYVoll",
        "outputId": "2645e741-6ec7-4381-ac0e-6d29d14e0ecb"
      },
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function _BaseOptimizer._update_step_xla at 0x7fb4cf3220e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 6 calls to <function _BaseOptimizer._update_step_xla at 0x7fb4cf3220e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "135/135 [==============================] - 37s 243ms/step - loss: 2.7844\n",
            "Epoch 2/20\n",
            "135/135 [==============================] - 33s 233ms/step - loss: 2.0137\n",
            "Epoch 3/20\n",
            "135/135 [==============================] - 31s 220ms/step - loss: 1.6017\n",
            "Epoch 4/20\n",
            "135/135 [==============================] - 32s 225ms/step - loss: 1.3423\n",
            "Epoch 5/20\n",
            "135/135 [==============================] - 31s 219ms/step - loss: 1.1342\n",
            "Epoch 6/20\n",
            "135/135 [==============================] - 31s 222ms/step - loss: 0.9259\n",
            "Epoch 7/20\n",
            "135/135 [==============================] - 31s 221ms/step - loss: 0.7195\n",
            "Epoch 8/20\n",
            "135/135 [==============================] - 31s 219ms/step - loss: 0.5317\n",
            "Epoch 9/20\n",
            "135/135 [==============================] - 32s 223ms/step - loss: 0.3813\n",
            "Epoch 10/20\n",
            "135/135 [==============================] - 31s 219ms/step - loss: 0.2798\n",
            "Epoch 11/20\n",
            "135/135 [==============================] - 31s 217ms/step - loss: 0.2106\n",
            "Epoch 12/20\n",
            "135/135 [==============================] - 32s 223ms/step - loss: 0.1707\n",
            "Epoch 13/20\n",
            "135/135 [==============================] - 31s 217ms/step - loss: 0.1472\n",
            "Epoch 14/20\n",
            "135/135 [==============================] - 32s 223ms/step - loss: 0.1341\n",
            "Epoch 15/20\n",
            "135/135 [==============================] - 31s 217ms/step - loss: 0.1268\n",
            "Epoch 16/20\n",
            "135/135 [==============================] - 32s 220ms/step - loss: 0.1235\n",
            "Epoch 17/20\n",
            "135/135 [==============================] - 31s 220ms/step - loss: 0.1203\n",
            "Epoch 18/20\n",
            "135/135 [==============================] - 32s 219ms/step - loss: 0.1202\n",
            "Epoch 19/20\n",
            "135/135 [==============================] - 31s 220ms/step - loss: 0.1230\n",
            "Epoch 20/20\n",
            "135/135 [==============================] - 32s 219ms/step - loss: 0.1397\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKkD5M6eoSiN"
      },
      "source": [
        "## Generate text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "zk2WJ2-XjkGz",
        "outputId": "c88371a7-bcb1-461b-b1df-f52e17dbab37"
      },
      "source": [
        "tf.train.latest_checkpoint(checkpoint_dir)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'./training_checkpoints/ckpt_20'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LycQ-ot_jjyu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ddccb1a4-1312-44cc-fda9-1fda259f5e9c"
      },
      "source": [
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._iterations\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._learning_rate\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.1\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.2\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.3\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.4\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.5\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.6\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.7\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.8\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.9\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.10\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.11\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.12\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.13\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.14\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.15\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.16\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.17\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.18\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.19\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.20\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.21\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.22\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.23\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71xa6jnYVrAN",
        "outputId": "a449c90b-f1ce-4437-9ee4-97821c6999f8"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_2 (Embedding)     (None, None, 256)         28928     \n",
            "                                                                 \n",
            " gru_6 (GRU)                 (None, None, 1024)        3938304   \n",
            "                                                                 \n",
            " gru_7 (GRU)                 (None, None, 1024)        6297600   \n",
            "                                                                 \n",
            " gru_8 (GRU)                 (None, None, 1024)        6297600   \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, None, 113)         115825    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 16,678,257\n",
            "Trainable params: 16,678,257\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WvuwZBX5Ogfd"
      },
      "source": [
        "def generate_text(model, start_string):\n",
        "    # Evaluation step (generating text using the learned model)\n",
        "\n",
        "    # Number of characters to generate\n",
        "    num_generate = 500\n",
        "\n",
        "    # Converting our start string to numbers (vectorizing)\n",
        "    input_eval = [char2idx[s] for s in start_string]\n",
        "    input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "    # Empty string to store our results\n",
        "    text_generated = []\n",
        "\n",
        "    # Low temperature results in more predictable text.\n",
        "    # Higher temperature results in more surprising text.\n",
        "    # Experiment to find the best setting.\n",
        "    temperature = 1\n",
        "\n",
        "    # Here batch size == 1\n",
        "    model.reset_states()\n",
        "    for i in range(num_generate):\n",
        "        predictions = model(input_eval)\n",
        "        predictions = tf.squeeze(predictions, 0)\n",
        "        # using a categorical distribution to predict the character returned by the model\n",
        "        predictions = predictions / temperature\n",
        "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
        "\n",
        "        # Pass the predicted character as the next input to the model\n",
        "        # along with the previous hidden state\n",
        "        input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "        text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "    return (start_string + ''.join(text_generated))"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ktovv0RFhrkn",
        "outputId": "06952a14-2086-4d0c-8c05-a814e884e1ab"
      },
      "source": [
        "text_ = generate_text(model, start_string=u\"Лес расступился,   мы   переехали   через  мост  и \")\n",
        "print(text_)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Лес расступился,   мы   переехали   через  мост  и   скух  созажедиехомиленари аму.  пакох злисколо. ологалонил  ги. ву   И ни,\r\n",
            "в. Дес?** дия.\r\n",
            "даманиви бро и\r\n",
            "от. Арай   Хозничегозавось дия  дил    перудржеся нестично     прегамиво   Янаст но\r\n",
            " рный?\r\n",
            "   ниль паболерегоце     пнко   испунобакцывамянно вогди  ого   тозалий.  слыйскт Я к\r\n",
            "льшимитемичичашет сиянил   ивзажудимно?\r\n",
            "оетьктопрнога    Пр.\r\n",
            "   скри    ий     сьнол\r\n",
            "пабря\"Поврицугротомо     я. легрыви  сяк. ожи    причи Онкалорисн.\r\n",
            "всмушегр-----и преск  --  Яни   ССл динося идобубъькар\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wqVniuFpofL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f549e50f-1ecf-4242-8fa9-42b55416e9d5"
      },
      "source": [
        "len(text_)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "926"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjuhCdppiVqy"
      },
      "source": [],
      "execution_count": 34,
      "outputs": []
    }
  ]
}