{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# !pip install transformers\n! pip install -U git+https://github.com/huggingface/transformers.git\n! pip install -U git+https://github.com/huggingface/accelerate.git","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### –ó–∞–¥–∞–Ω–∏–µ 14\n–ó–∞–¥–∞–Ω–∏–µ –≤–∑—è—Ç—å –¥–∞–Ω–Ω—ã–µ –∏–∑ https://www.kaggle.com/datasets/mrapplexz/bashim-quotes –æ–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å GPT –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–≤–æ–∏—Ö —Ü–∏—Ç–∞—Ç\n\n–≤–∑—è—Ç—å –Ω–æ–≤–æ—Å—Ç–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ https://github.com/natasha/corus load_lenta2 –Ω–∞–º –ø–æ–Ω–∞–¥–æ–±–∏—Ç—å—Å—è —Å–∞–º —Ç–µ–∫—Å—Ç –∏ –∑–∞–≥–æ–ª–æ–≤–æ–∫ –æ–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å T5/ –∏–ª–∏ GPT –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –¥–ª—è —Å—Ç–∞—Ç–µ–π","metadata":{}},{"cell_type":"code","source":"import pandas as pd","metadata":{"execution":{"iopub.status.busy":"2023-06-01T19:16:32.657098Z","iopub.execute_input":"2023-06-01T19:16:32.658072Z","iopub.status.idle":"2023-06-01T19:16:32.668025Z","shell.execute_reply.started":"2023-06-01T19:16:32.658022Z","shell.execute_reply":"2023-06-01T19:16:32.667011Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"df_rec = pd.read_json('/kaggle/input/bashim-quotes/dataset.jsonl', lines=True).set_index('id')\ndf_rec.head()","metadata":{"execution":{"iopub.status.busy":"2023-06-01T19:16:35.128874Z","iopub.execute_input":"2023-06-01T19:16:35.129253Z","iopub.status.idle":"2023-06-01T19:16:36.278679Z","shell.execute_reply.started":"2023-06-01T19:16:35.129224Z","shell.execute_reply":"2023-06-01T19:16:36.277529Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"                        date   rating  \\\nid                                      \n1  2004-08-30 11:24:00+00:00  22010.0   \n2  2004-08-30 11:25:00+00:00  25105.0   \n3  2004-08-30 11:27:00+00:00   7192.0   \n4  2004-08-30 11:28:00+00:00  29169.0   \n5  2004-08-30 11:26:00+00:00   7140.0   \n\n                                                 text  \nid                                                     \n1   <Ares> ppdv, –≤—Å–µ —é–Ω–∏–∫—Å—ã –æ—á–µ–Ω—å –¥—Ä—É–∂–µ–ª—é–±–Ω—ã.. –æ–Ω–∏...  \n2   <—Ç–æ–º–∞—Ç–∏–∫_—Ä–∞–¥> –∞ —Ç—ã –Ω–µ —á—É–≤—Å—Ç–≤—É–µ—à—å –∫—Ä–∞—Å–æ—Ç—É –º–∏—Ä–∞?...  \n3   <–î–æ—Ä> \"–º—ã—à–∫–∞, –ø–æ—á–µ–º—É —É —Ç–µ–±—è —Ç–∞–∫–∏–µ –±–æ–ª—å—à–∏–µ –≥–ª–∞–∑...  \n4   <PPDV[os2]> \"–ú–∞–ª—å—á–∏–∫–∏, –≤—ã —á—Ç–æ –±–æ–ª—å–Ω—ã–µ, –±–µ–≥–∞—Ç—å ...  \n5   <Ohtori_Akio> –º—ã - –∫–∞–∫ —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–∏ - –∂–∏–≤—ë–º —Å ...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>date</th>\n      <th>rating</th>\n      <th>text</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>2004-08-30 11:24:00+00:00</td>\n      <td>22010.0</td>\n      <td>&lt;Ares&gt; ppdv, –≤—Å–µ —é–Ω–∏–∫—Å—ã –æ—á–µ–Ω—å –¥—Ä—É–∂–µ–ª—é–±–Ω—ã.. –æ–Ω–∏...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2004-08-30 11:25:00+00:00</td>\n      <td>25105.0</td>\n      <td>&lt;—Ç–æ–º–∞—Ç–∏–∫_—Ä–∞–¥&gt; –∞ —Ç—ã –Ω–µ —á—É–≤—Å—Ç–≤—É–µ—à—å –∫—Ä–∞—Å–æ—Ç—É –º–∏—Ä–∞?...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2004-08-30 11:27:00+00:00</td>\n      <td>7192.0</td>\n      <td>&lt;–î–æ—Ä&gt; \"–º—ã—à–∫–∞, –ø–æ—á–µ–º—É —É —Ç–µ–±—è —Ç–∞–∫–∏–µ –±–æ–ª—å—à–∏–µ –≥–ª–∞–∑...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2004-08-30 11:28:00+00:00</td>\n      <td>29169.0</td>\n      <td>&lt;PPDV[os2]&gt; \"–ú–∞–ª—å—á–∏–∫–∏, –≤—ã —á—Ç–æ –±–æ–ª—å–Ω—ã–µ, –±–µ–≥–∞—Ç—å ...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>2004-08-30 11:26:00+00:00</td>\n      <td>7140.0</td>\n      <td>&lt;Ohtori_Akio&gt; –º—ã - –∫–∞–∫ —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–∏ - –∂–∏–≤—ë–º —Å ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"import re\nfrom sklearn.model_selection import train_test_split\n\ndef build_text_files(data_json, dest_path):\n    f = open(dest_path, 'w')\n    data = ''\n    for texts in data_json:\n        summary = str(texts).strip()\n        summary = re.sub(r\"\\s\", \" \", summary)\n        data += summary + \"  \"\n    f.write(data)","metadata":{"execution":{"iopub.status.busy":"2023-06-01T19:16:36.489781Z","iopub.execute_input":"2023-06-01T19:16:36.490435Z","iopub.status.idle":"2023-06-01T19:16:36.992781Z","shell.execute_reply.started":"2023-06-01T19:16:36.490398Z","shell.execute_reply":"2023-06-01T19:16:36.991868Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"data = df_rec.loc[:5000, 'text']","metadata":{"execution":{"iopub.status.busy":"2023-06-01T19:16:40.563002Z","iopub.execute_input":"2023-06-01T19:16:40.563534Z","iopub.status.idle":"2023-06-01T19:16:40.580119Z","shell.execute_reply.started":"2023-06-01T19:16:40.563492Z","shell.execute_reply":"2023-06-01T19:16:40.579100Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"train, test = train_test_split(data, test_size=0.15)\n\nbuild_text_files(train,'train_dataset.txt')\nbuild_text_files(test,'test_dataset.txt')","metadata":{"execution":{"iopub.status.busy":"2023-06-01T19:16:42.856707Z","iopub.execute_input":"2023-06-01T19:16:42.857073Z","iopub.status.idle":"2023-06-01T19:16:42.886699Z","shell.execute_reply.started":"2023-06-01T19:16:42.857028Z","shell.execute_reply":"2023-06-01T19:16:42.885883Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"print(\"Train dataset length: \"+ str(len(train)))\nprint(\"Test dataset length: \"+ str(len(test)))","metadata":{"execution":{"iopub.status.busy":"2023-06-01T19:16:43.833326Z","iopub.execute_input":"2023-06-01T19:16:43.833694Z","iopub.status.idle":"2023-06-01T19:16:43.840019Z","shell.execute_reply.started":"2023-06-01T19:16:43.833665Z","shell.execute_reply":"2023-06-01T19:16:43.838608Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Train dataset length: 1197\nTest dataset length: 212\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import AutoTokenizer\n#sberbank-ai/rugpt3large_based_on_gpt2\n#sberbank-ai/rugpt3medium_based_on_gpt2\n#sberbank-ai/rugpt3small_based_on_gpt2\n\ntokenizer = AutoTokenizer.from_pretrained(\"sberbank-ai/rugpt3small_based_on_gpt2\")\n\ntrain_path = 'train_dataset.txt'\ntest_path = 'test_dataset.txt'","metadata":{"execution":{"iopub.status.busy":"2023-06-01T19:16:46.300670Z","iopub.execute_input":"2023-06-01T19:16:46.301521Z","iopub.status.idle":"2023-06-01T19:16:47.867626Z","shell.execute_reply.started":"2023-06-01T19:16:46.301476Z","shell.execute_reply":"2023-06-01T19:16:47.866520Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import TextDataset, DataCollatorForLanguageModeling\n\ndef load_dataset(train_path, test_path, tokenizer):\n    train_dataset = TextDataset(\n          tokenizer=tokenizer,\n          file_path=train_path,\n          block_size=128)\n\n    test_dataset = TextDataset(\n          tokenizer=tokenizer,\n          file_path=test_path,\n          block_size=128)\n\n    data_collator = DataCollatorForLanguageModeling(\n        tokenizer=tokenizer, mlm=False,\n    )\n    return train_dataset, test_dataset, data_collator\n\ntrain_dataset, test_dataset, data_collator = load_dataset(train_path, test_path, tokenizer)\n","metadata":{"execution":{"iopub.status.busy":"2023-06-01T19:16:50.485280Z","iopub.execute_input":"2023-06-01T19:16:50.485830Z","iopub.status.idle":"2023-06-01T19:16:54.827213Z","shell.execute_reply.started":"2023-06-01T19:16:50.485792Z","shell.execute_reply":"2023-06-01T19:16:54.826090Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n/opt/conda/lib/python3.10/site-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ü§ó Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import Trainer, TrainingArguments, AutoModelForCausalLM\n\nmodel = AutoModelForCausalLM.from_pretrained(\"sberbank-ai/rugpt3small_based_on_gpt2\")","metadata":{"execution":{"iopub.status.busy":"2023-06-01T19:16:56.833381Z","iopub.execute_input":"2023-06-01T19:16:56.834112Z","iopub.status.idle":"2023-06-01T19:17:03.687416Z","shell.execute_reply.started":"2023-06-01T19:16:56.834069Z","shell.execute_reply":"2023-06-01T19:17:03.686429Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/551M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3e86467efdd44512a49e75218ea84e65"}},"metadata":{}}]},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir=\"./gpt2-chief\", #The output directory\n    overwrite_output_dir=True, #overwrite the content of the output directory\n    num_train_epochs=3, # number of training epochs\n    per_device_train_batch_size=4, # batch size for training\n    per_device_eval_batch_size=4,  # batch size for evaluation\n    eval_steps = 400, # Number of update steps between two evaluations.\n    save_steps=800, # after # steps model is saved\n    warmup_steps=500,# number of warmup steps for learning rate scheduler\n    )","metadata":{"execution":{"iopub.status.busy":"2023-06-01T19:17:17.408837Z","iopub.execute_input":"2023-06-01T19:17:17.409539Z","iopub.status.idle":"2023-06-01T19:17:17.498589Z","shell.execute_reply.started":"2023-06-01T19:17:17.409507Z","shell.execute_reply":"2023-06-01T19:17:17.497684Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"trainer = Trainer(\n    model=model,\n    args=training_args,\n    data_collator=data_collator,\n    train_dataset=train_dataset,\n    eval_dataset=test_dataset\n)","metadata":{"execution":{"iopub.status.busy":"2023-06-01T19:17:33.354517Z","iopub.execute_input":"2023-06-01T19:17:33.355011Z","iopub.status.idle":"2023-06-01T19:17:39.174864Z","shell.execute_reply.started":"2023-06-01T19:17:33.354981Z","shell.execute_reply":"2023-06-01T19:17:39.173906Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2023-06-01T19:17:41.753413Z","iopub.execute_input":"2023-06-01T19:17:41.753760Z","iopub.status.idle":"2023-06-01T19:22:28.307835Z","shell.execute_reply.started":"2023-06-01T19:17:41.753731Z","shell.execute_reply":"2023-06-01T19:22:28.305606Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.15.3"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20230601_192042-0wsz10no</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/alx-illarionov/huggingface/runs/0wsz10no' target=\"_blank\">cosmic-meadow-1</a></strong> to <a href='https://wandb.ai/alx-illarionov/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/alx-illarionov/huggingface' target=\"_blank\">https://wandb.ai/alx-illarionov/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/alx-illarionov/huggingface/runs/0wsz10no' target=\"_blank\">https://wandb.ai/alx-illarionov/huggingface/runs/0wsz10no</a>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='222' max='222' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [222/222 01:07, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=222, training_loss=4.141312057907517, metrics={'train_runtime': 286.5237, 'train_samples_per_second': 6.177, 'train_steps_per_second': 0.775, 'total_flos': 115621724160000.0, 'train_loss': 4.141312057907517, 'epoch': 3.0})"},"metadata":{}}]},{"cell_type":"code","source":"trainer.save_model()","metadata":{"execution":{"iopub.status.busy":"2023-06-01T19:22:41.010201Z","iopub.execute_input":"2023-06-01T19:22:41.010974Z","iopub.status.idle":"2023-06-01T19:22:42.012767Z","shell.execute_reply.started":"2023-06-01T19:22:41.010939Z","shell.execute_reply":"2023-06-01T19:22:42.002974Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"tokenizer.save_pretrained('gpt_chf')","metadata":{"execution":{"iopub.status.busy":"2023-06-01T19:23:06.570873Z","iopub.execute_input":"2023-06-01T19:23:06.571231Z","iopub.status.idle":"2023-06-01T19:23:06.729367Z","shell.execute_reply.started":"2023-06-01T19:23:06.571200Z","shell.execute_reply":"2023-06-01T19:23:06.728039Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"('gpt_chf/tokenizer_config.json',\n 'gpt_chf/special_tokens_map.json',\n 'gpt_chf/vocab.json',\n 'gpt_chf/merges.txt',\n 'gpt_chf/added_tokens.json',\n 'gpt_chf/tokenizer.json')"},"metadata":{}}]},{"cell_type":"code","source":"tokenizer.save_pretrained('gpt_chf')\nmodel.save_pretrained('model_gpt_chf')","metadata":{"execution":{"iopub.status.busy":"2023-06-01T19:23:19.226757Z","iopub.execute_input":"2023-06-01T19:23:19.227467Z","iopub.status.idle":"2023-06-01T19:23:20.250186Z","shell.execute_reply.started":"2023-06-01T19:23:19.227435Z","shell.execute_reply":"2023-06-01T19:23:20.248930Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"gpt_chf\")\nmodel1 = AutoModelForCausalLM.from_pretrained(\"model_gpt_chf\")","metadata":{"execution":{"iopub.status.busy":"2023-06-01T19:23:30.397414Z","iopub.execute_input":"2023-06-01T19:23:30.397763Z","iopub.status.idle":"2023-06-01T19:23:32.706507Z","shell.execute_reply.started":"2023-06-01T19:23:30.397732Z","shell.execute_reply":"2023-06-01T19:23:32.705427Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"prefix = \"–ï—Ö–∞–ª–∏ –º–µ–¥–≤–µ–¥–∏ –Ω–∞ –≤–µ–ª–æ—Å–∏–ø–µ–¥–µ \"","metadata":{"execution":{"iopub.status.busy":"2023-06-01T19:26:19.942679Z","iopub.execute_input":"2023-06-01T19:26:19.943701Z","iopub.status.idle":"2023-06-01T19:26:19.950546Z","shell.execute_reply.started":"2023-06-01T19:26:19.943656Z","shell.execute_reply":"2023-06-01T19:26:19.949495Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"tokens = tokenizer(prefix, return_tensors='pt')\n#tokens = {k: v.to(model.device) for k, v in tokens.items()}","metadata":{"execution":{"iopub.status.busy":"2023-06-01T19:26:22.969766Z","iopub.execute_input":"2023-06-01T19:26:22.970136Z","iopub.status.idle":"2023-06-01T19:26:22.977468Z","shell.execute_reply.started":"2023-06-01T19:26:22.970105Z","shell.execute_reply":"2023-06-01T19:26:22.976479Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"size = tokens['input_ids'].shape[1]\noutput = model1.generate(\n    **tokens, \n    #end_token=end_token_id,\n    do_sample=False, \n    max_length=size+50, \n    repetition_penalty=5., \n    temperature=0.5,\n    num_beams=10,\n)\n\ndecoded = tokenizer.decode(output[0])\nresult = decoded[len(prefix):]\nprint(prefix + result)","metadata":{"execution":{"iopub.status.busy":"2023-06-01T19:26:24.334256Z","iopub.execute_input":"2023-06-01T19:26:24.334609Z","iopub.status.idle":"2023-06-01T19:26:32.724725Z","shell.execute_reply.started":"2023-06-01T19:26:24.334580Z","shell.execute_reply":"2023-06-01T19:26:32.723735Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"–ï—Ö–∞–ª–∏ –º–µ–¥–≤–µ–¥–∏ –Ω–∞ –≤–µ–ª–æ—Å–∏–ø–µ–¥–µ  <maxtar[away]> –∞ –∫—Ç–æ-–Ω–∏—Ç—å –∑–Ω–∞–µ—Ç, –≥–¥–µ –≤ –ú–æ—Å–∫–≤–µ –º–æ–∂–Ω–æ –∫—É–ø–∏—Ç—å —Ç–∞–∫—É—é —à—Ç—É–∫—É? <Maxtar[away]> –ê —Ç–æ —É –º–µ–Ω—è –µ—Å—Ç—å –æ–¥–∏–Ω –∑–Ω–∞–∫–æ–º—ã–π, –∫–æ—Ç–æ—Ä—ã–π –∑–∞–Ω–∏–º–∞–µ—Ç—Å—è –ø—Ä–æ–¥–∞–∂–µ–π –≤–µ–ª–æ—Å–∏–ø–µ–¥–æ–≤. –û–Ω\n","output_type":"stream"}]}]}