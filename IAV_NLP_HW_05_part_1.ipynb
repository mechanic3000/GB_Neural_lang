{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45ffb3e2",
   "metadata": {},
   "source": [
    "### Тема «POS-tagger и NER»\n",
    "\n",
    "*Задание 1.* Написать теггер на данных с русским языком\n",
    "* проверить UnigramTagger, BigramTagger, TrigramTagger и их комбинации\n",
    "* написать свой теггер как на занятии, попробовать разные векторайзеры, добавить знание не только букв но и слов\n",
    "* сравнить все реализованные методы, сделать выводы  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a450ca3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyconll\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f71d0a09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-04-20 17:36:00--  https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-train-a.conllu\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 40736581 (39M) [text/plain]\n",
      "Saving to: './dataset/ru_syntagrus-ud-train-a.conllu'\n",
      "\n",
      "./dataset/ru_syntag 100%[===================>]  38.85M  5.88MB/s    in 5.7s    \n",
      "\n",
      "2023-04-20 17:36:08 (6.80 MB/s) - './dataset/ru_syntagrus-ud-train-a.conllu' saved [40736581/40736581]\n",
      "\n",
      "--2023-04-20 17:36:08--  https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-train-b.conllu\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 42819832 (41M) [text/plain]\n",
      "Saving to: './dataset/ru_syntagrus-ud-train-b.conllu'\n",
      "\n",
      "./dataset/ru_syntag 100%[===================>]  40.84M  2.43MB/s    in 17s     \n",
      "\n",
      "2023-04-20 17:36:28 (2.35 MB/s) - './dataset/ru_syntagrus-ud-train-b.conllu' saved [42819832/42819832]\n",
      "\n",
      "--2023-04-20 17:36:28--  https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-train-c.conllu\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 32367510 (31M) [text/plain]\n",
      "Saving to: './dataset/ru_syntagrus-ud-train-c.conllu'\n",
      "\n",
      "./dataset/ru_syntag 100%[===================>]  30.87M  3.27MB/s    in 14s     \n",
      "\n",
      "2023-04-20 17:36:45 (2.20 MB/s) - './dataset/ru_syntagrus-ud-train-c.conllu' saved [32367510/32367510]\n",
      "\n",
      "--2023-04-20 17:36:45--  https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-dev.conllu\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 14704579 (14M) [text/plain]\n",
      "Saving to: './dataset/ru_syntagrus-ud-dev.conllu'\n",
      "\n",
      "./dataset/ru_syntag 100%[===================>]  14.02M  4.59MB/s    in 3.1s    \n",
      "\n",
      "2023-04-20 17:36:48 (4.59 MB/s) - './dataset/ru_syntagrus-ud-dev.conllu' saved [14704579/14704579]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -O ./dataset/ru_syntagrus-ud-train-a.conllu https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-train-a.conllu\n",
    "!wget -O ./dataset/ru_syntagrus-ud-train-b.conllu https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-train-b.conllu\n",
    "!wget -O ./dataset/ru_syntagrus-ud-train-c.conllu https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-train-c.conllu\n",
    "!wget -O ./dataset/ru_syntagrus-ud-dev.conllu https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-dev.conllu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47a2140e",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_train = pyconll.load_from_file('dataset/ru_syntagrus-ud-train-a.conllu')\n",
    "full_train_b = pyconll.load_from_file('dataset/ru_syntagrus-ud-train-b.conllu')\n",
    "full_train_c = pyconll.load_from_file('dataset/ru_syntagrus-ud-train-c.conllu')\n",
    "\n",
    "# Общая обучающая выборка\n",
    "full_train.extend([*full_train_b, *full_train_c])\n",
    "\n",
    "\n",
    "full_test = pyconll.load_from_file('dataset/ru_syntagrus-ud-dev.conllu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a844d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Анкета NOUN\n",
      ". PUNCT\n",
      "\n",
      "Начальник NOUN\n",
      "областного ADJ\n",
      "управления NOUN\n",
      "связи NOUN\n",
      "Семен PROPN\n",
      "Еремеевич PROPN\n",
      "был AUX\n",
      "человек NOUN\n",
      "простой ADJ\n",
      ", PUNCT\n",
      "приходил VERB\n",
      "на ADP\n",
      "работу NOUN\n",
      "всегда ADV\n",
      "вовремя ADV\n",
      ", PUNCT\n",
      "здоровался VERB\n",
      "с ADP\n",
      "секретаршей NOUN\n",
      "за ADP\n",
      "руку NOUN\n",
      "и CCONJ\n",
      "иногда ADV\n",
      "даже PART\n",
      "писал VERB\n",
      "в ADP\n",
      "стенгазету NOUN\n",
      "заметки NOUN\n",
      "под ADP\n",
      "псевдонимом NOUN\n",
      "\" PUNCT\n",
      "Муха NOUN\n",
      "\" PUNCT\n",
      ". PUNCT\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sent in full_train[:2]:\n",
    "    for token in sent:\n",
    "        print(token.form, token.upos)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5538a10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fdata_train = []\n",
    "for sent in full_train[:]:\n",
    "    fdata_train.append([(str(token.form), str(token.upos)) for token in sent])\n",
    "    \n",
    "fdata_test = []\n",
    "for sent in full_test[:]:\n",
    "    fdata_test.append([(str(token.form), str(token.upos)) for token in sent])\n",
    "    \n",
    "fdata_sent_test = []\n",
    "for sent in full_test[:]:\n",
    "    fdata_sent_test.append([str(token.form) for token in sent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5389c533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Наибольшая длина предложения 205\n",
      "Наибольшая длина токена 47\n"
     ]
    }
   ],
   "source": [
    "MAX_SENT_LEN = max(len(sent) for sent in full_train)\n",
    "MAX_ORIG_TOKEN_LEN = max(len(str(token.form)) for sent in full_train for token in sent)\n",
    "print('Наибольшая длина предложения', MAX_SENT_LEN)\n",
    "print('Наибольшая длина токена', MAX_ORIG_TOKEN_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5ec47ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Анкета .\n",
      "Начальник областного управления связи Семен Еремеевич был человек простой , приходил на работу всегда вовремя , здоровался с секретаршей за руку и иногда даже писал в стенгазету заметки под псевдонимом \" Муха \" .\n",
      "В приемной его с утра ожидали посетители , - кое-кто с важными делами , а кое-кто и с такими , которые легко можно было решить в нижестоящих инстанциях , не затрудняя Семена Еремеевича .\n",
      "Однако стиль работы Семена Еремеевича заключался в том , чтобы принимать всех желающих и лично вникать в дело .\n",
      "Приемная была обставлена просто , но по-деловому .\n",
      "У двери стоял стол секретарши , на столе - пишущая машинка с широкой кареткой .\n",
      "В углу висел репродуктор и играло радио для развлечения ожидающих и еще для того , чтобы заглушать голос начальника , доносившийся из кабинета , так как , бесспорно , среди посетителей могли находиться и случайные люди .\n",
      "Кабинет отличался скромностью , присущей Семену Еремеевичу .\n",
      "В глубине стоял широкий письменный стол с бронзовыми чернильницами и перед ним два кожаных кресла .\n",
      "Справа был стол для заседаний - длинный , накрытый зеленым сукном и с обеих сторон аккуратно заставленный стульями .\n"
     ]
    }
   ],
   "source": [
    "all_train_texts = [' '.join(str(token.form) for token in sent) for sent in full_train]\n",
    "all_test_texts = [' '.join(str(token.form) for token in sent) for sent in full_test]\n",
    "\n",
    "all_train_labels = [' '.join(str(token.form) for token in sent) for sent in full_train]\n",
    "all_test_labels = [' '.join(str(token.form) for token in sent) for sent in full_test]\n",
    "print('\\n'.join(all_train_texts[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ea98f02a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Это', 'PRON'),\n",
       " ('сочинение', None),\n",
       " ('известно', None),\n",
       " ('во', 'ADP'),\n",
       " ('многих', 'NUM'),\n",
       " ('вариантах', 'NOUN'),\n",
       " ('(', 'PUNCT'),\n",
       " ('самые', 'ADJ'),\n",
       " ('ранние', 'ADJ'),\n",
       " ('из', 'ADP'),\n",
       " ('них', 'PRON'),\n",
       " ('почти', 'ADV'),\n",
       " ('на', 'ADP'),\n",
       " ('сто', 'NUM'),\n",
       " ('лет', 'NOUN'),\n",
       " ('старше', 'ADJ'),\n",
       " (')', 'PUNCT'),\n",
       " ('и', 'CCONJ'),\n",
       " ('восходит', None),\n",
       " ('к', 'ADP'),\n",
       " ('ещё', None),\n",
       " ('более', 'ADV'),\n",
       " ('древним', None),\n",
       " ('рукописям', None),\n",
       " ('XVI', None),\n",
       " ('в', 'ADP'),\n",
       " ('.', 'PUNCT')]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.7101308678950452"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bigram_tagger = BigramTagger(fdata_train)\n",
    "display(bigram_tagger.tag(fdata_sent_test[100]), bigram_tagger.evaluate(fdata_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "925d0ff3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Это', 'PRON'),\n",
       " ('сочинение', 'NOUN'),\n",
       " ('известно', 'ADJ'),\n",
       " ('во', 'ADP'),\n",
       " ('многих', 'NUM'),\n",
       " ('вариантах', 'NOUN'),\n",
       " ('(', 'PUNCT'),\n",
       " ('самые', 'ADJ'),\n",
       " ('ранние', 'ADJ'),\n",
       " ('из', 'ADP'),\n",
       " ('них', 'PRON'),\n",
       " ('почти', 'ADV'),\n",
       " ('на', 'ADP'),\n",
       " ('сто', 'NUM'),\n",
       " ('лет', 'NOUN'),\n",
       " ('старше', 'ADJ'),\n",
       " (')', 'PUNCT'),\n",
       " ('и', 'CCONJ'),\n",
       " ('восходит', 'VERB'),\n",
       " ('к', 'ADP'),\n",
       " ('ещё', 'PART'),\n",
       " ('более', 'ADV'),\n",
       " ('древним', 'ADJ'),\n",
       " ('рукописям', 'NOUN'),\n",
       " ('XVI', 'ADJ'),\n",
       " ('в', 'ADP'),\n",
       " ('.', 'PUNCT')]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.8782863467673677"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "unigram_tagger = UnigramTagger(fdata_train)\n",
    "display(unigram_tagger.tag(fdata_sent_test[100]), unigram_tagger.evaluate(fdata_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5694ecae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Это', 'PRON'),\n",
       " ('сочинение', None),\n",
       " ('известно', None),\n",
       " ('во', 'ADP'),\n",
       " ('многих', None),\n",
       " ('вариантах', None),\n",
       " ('(', None),\n",
       " ('самые', None),\n",
       " ('ранние', None),\n",
       " ('из', 'ADP'),\n",
       " ('них', 'PRON'),\n",
       " ('почти', 'ADV'),\n",
       " ('на', 'ADP'),\n",
       " ('сто', None),\n",
       " ('лет', None),\n",
       " ('старше', None),\n",
       " (')', None),\n",
       " ('и', 'PART'),\n",
       " ('восходит', None),\n",
       " ('к', None),\n",
       " ('ещё', None),\n",
       " ('более', None),\n",
       " ('древним', None),\n",
       " ('рукописям', None),\n",
       " ('XVI', None),\n",
       " ('в', 'ADP'),\n",
       " ('.', None)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.4067191874470994"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trigram_tagger = TrigramTagger(fdata_train)\n",
    "display(trigram_tagger.tag(fdata_sent_test[100]), trigram_tagger.evaluate(fdata_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1b1872b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Это', 'PRON'),\n",
       " ('сочинение', None),\n",
       " ('известно', None),\n",
       " ('во', 'ADP'),\n",
       " ('многих', 'NUM'),\n",
       " ('вариантах', 'NOUN'),\n",
       " ('(', 'PUNCT'),\n",
       " ('самые', 'ADJ'),\n",
       " ('ранние', 'ADJ'),\n",
       " ('из', 'ADP'),\n",
       " ('них', 'PRON'),\n",
       " ('почти', 'ADV'),\n",
       " ('на', 'ADP'),\n",
       " ('сто', 'NUM'),\n",
       " ('лет', 'NOUN'),\n",
       " ('старше', 'ADJ'),\n",
       " (')', 'PUNCT'),\n",
       " ('и', 'CCONJ'),\n",
       " ('восходит', None),\n",
       " ('к', 'ADP'),\n",
       " ('ещё', 'PART'),\n",
       " ('более', 'ADV'),\n",
       " ('древним', None),\n",
       " ('рукописям', None),\n",
       " ('XVI', 'ADJ'),\n",
       " ('в', 'ADP'),\n",
       " ('.', 'PUNCT')]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.7160166677518067"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "unigram_bigram_tagger = UnigramTagger(fdata_train, backoff=bigram_tagger)\n",
    "display(unigram_tagger.tag(fdata_sent_test[100]), unigram_tagger.evaluate(fdata_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "670446e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Это', 'PRON'),\n",
       " ('сочинение', 'NOUN'),\n",
       " ('известно', 'ADJ'),\n",
       " ('во', 'ADP'),\n",
       " ('многих', 'NUM'),\n",
       " ('вариантах', 'NOUN'),\n",
       " ('(', 'PUNCT'),\n",
       " ('самые', 'ADJ'),\n",
       " ('ранние', 'ADJ'),\n",
       " ('из', 'ADP'),\n",
       " ('них', 'PRON'),\n",
       " ('почти', 'ADV'),\n",
       " ('на', 'ADP'),\n",
       " ('сто', 'NUM'),\n",
       " ('лет', 'NOUN'),\n",
       " ('старше', 'ADJ'),\n",
       " (')', 'PUNCT'),\n",
       " ('и', 'CCONJ'),\n",
       " ('восходит', 'VERB'),\n",
       " ('к', 'ADP'),\n",
       " ('ещё', 'PART'),\n",
       " ('более', 'ADV'),\n",
       " ('древним', 'ADJ'),\n",
       " ('рукописям', 'NOUN'),\n",
       " ('XVI', 'ADJ'),\n",
       " ('в', 'ADP'),\n",
       " ('.', 'PUNCT')]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.8839768214076438"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bigram_unigram_tagger = BigramTagger(fdata_train, backoff=unigram_tagger)\n",
    "display(bigram_tagger.tag(fdata_sent_test[100]), bigram_tagger.evaluate(fdata_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cda7348",
   "metadata": {},
   "source": [
    "Попробуем комбинацию тэггеров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "9eb6d722",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.843317053722715"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def backoff_tagger(train_sents, tagger_classes, backoff=None):\n",
    "    for cls in tagger_classes:\n",
    "        backoff = cls(train_sents, backoff=backoff)\n",
    "    return backoff\n",
    "\n",
    "\n",
    "backoff = DefaultTagger('NN') \n",
    "tag = backoff_tagger(train_data,  \n",
    "                     [UnigramTagger, BigramTagger, TrigramTagger],  \n",
    "                     backoff = backoff) \n",
    "  \n",
    "tag.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97daa051",
   "metadata": {},
   "source": [
    "Для данной задачи максимальную оценку показала комбинация bigram + unigram tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0afdef62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30b9dfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tok = []\n",
    "train_label = []\n",
    "for sent in fdata_train[:]:\n",
    "    for tok in sent:\n",
    "        train_tok.append(tok[0])\n",
    "        train_label.append('NO_TAG' if tok[1] is None else tok[1])\n",
    "        \n",
    "test_tok = []\n",
    "test_label = []\n",
    "for sent in fdata_test[:]:\n",
    "    for tok in sent:\n",
    "        test_tok.append(tok[0])\n",
    "        test_label.append('NO_TAG' if tok[1] is None else tok[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dabde586",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "train_enc_labels = le.fit_transform(train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00e7d3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_enc_labels = le.transform(test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c406a1e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ADJ', 'ADP', 'ADV', 'AUX', 'CCONJ', 'DET', 'INTJ', 'NOUN', 'NUM',\n",
       "       'None', 'PART', 'PRON', 'PROPN', 'PUNCT', 'SCONJ', 'SYM', 'VERB',\n",
       "       'X'], dtype='<U5')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f2d092a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(le.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "19e99d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "hvectorizer = HashingVectorizer(ngram_range=(1, 3), analyzer='char', n_features=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1ba62a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = hvectorizer.fit_transform(train_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "00951fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = hvectorizer.transform(test_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "14cec015",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1206302, 500)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "129558a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexey/opt/anaconda3/envs/ml_3_9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=10, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=10, random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(max_iter=10, random_state=42)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression(random_state=42, max_iter=10)\n",
    "lr.fit(X_train, train_enc_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8c19fac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ba0abe54",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6093105019858064"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(test_enc_labels, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2612c9",
   "metadata": {},
   "source": [
    "Как видимо наибольшая точность получилась при использовании тэггера bigram_unigram_tagger"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_3_9",
   "language": "python",
   "name": "ml_3_9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
